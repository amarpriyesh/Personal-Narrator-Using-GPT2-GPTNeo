{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1024)\n    (wpe): Embedding(1024, 1024)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-23): 24 x GPT2Block(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, Trainer, TrainingArguments\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "\n",
    "\n",
    "# Make sure CUDA operations are set for debugging. Remove in production.\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Define paths\n",
    "train_data_file = \"./train.csv\"\n",
    "output_dir = \"./model_output\"\n",
    "\n",
    "# Check for the existence of the training data file\n",
    "if not os.path.isfile(train_data_file):\n",
    "    raise ValueError(f\"Training data file not found: {train_data_file}\")\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"gpt2-medium\"  # You can also use \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Resize model embeddings to match the new tokenizer size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move model to the device\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amarp\\.conda\\envs\\MLClass\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/900 : < :, Epoch 0.01/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_data_file = \"./eval.csv\"\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_data_file,\n",
    "    block_size=128  # Adjusted block size, ensure it's less than the model's max input length\n",
    ")\n",
    "\n",
    "eval_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=eval_data_file,\n",
    "    block_size=128  # Make sure this is smaller than the model's max input length\n",
    ")\n",
    "\n",
    "# Prepare data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments with additional optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # Reduced for a small dataset\n",
    "    per_device_train_batch_size=2,  # Keep the reduced batch size for memory management\n",
    "    save_strategy=\"epoch\",  # Save at the end of each epoch\n",
    "    logging_steps=1,  # Log more frequently due to fewer steps per epoch\n",
    "    gradient_accumulation_steps=1,  # Adjust as needed\n",
    "    fp16=True,  # Enable mixed precision if your GPU supports it\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer with compute_metrics function for accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #compute_metrics=compute_metrics  # Compute accuracy per epoch\n",
    ")\n",
    "\n",
    "# Train and save the model\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 1: Tell me something about yourself?,\"I am a software engineer by training. I am passionate about technology and have a keen interest in web technologies. My passion for technology led me to pursue a Master's in Computer Science from Northeastern University.\"What is\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    " # Replace with your actual model directory\n",
    "model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
    "model.to(device)\n",
    "\n",
    "def generate_text(prompt, max_length=50, temperature=0.5, top_k=50, top_p=0.5, num_beams=10, early_stopping=True, no_repeat_ngram_size=2, do_sample=True, num_return_sequences=1):\n",
    "    # Encode the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    generated_outputs = model.generate(\n",
    "        input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=early_stopping,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        do_sample=do_sample,\n",
    "        num_return_sequences=num_return_sequences\n",
    "    )\n",
    "\n",
    "    # Decode and print generated text\n",
    "    for i, output in enumerate(generated_outputs):\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        print(f\"Generated text {i+1}: {generated_text}\")\n",
    "\n",
    "# Text Generation Loop\n",
    "while True:\n",
    "    scan = input(\"Enter prompt (or 'Exit' to quit): \").strip()\n",
    "    if scan.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Generate and print text\n",
    "    generate_text(scan)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text 1: Tell me sokmething about Priyesh?\",\"Priyashree is a software engineer by training. He is passionate about technology and software engineering, and is dedicated to improving the quality of life for others.\"What is your approach to continuous\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    scan = input(\"Enter prompt (or Exit' to quit): \").strip()\n",
    "    if scan.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Generate and print text\n",
    "    generate_text(scan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://huggingface.co/priyesh2023/GPT\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"Who is priyesh?\",\n",
    "    \"parameters\": {\n",
    "        \"max_length\": 100,\n",
    "        \"temperature\": 0.6,\n",
    "        \"top_k\": 40,\n",
    "        \"top_p\": 0.5,\n",
    "        \"num_beams\": 5,\n",
    "        \"early_stopping\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"do_sample\": True,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "result = response\n",
    "\n",
    "print(result)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What is Priyesh\\'s GPA? Priyaesh has a GPA of 3.9 at Northeastern University.\"Priyash is pursuing a Master of Science in Computer Science at Northwestern University, where he is working on projects related to machine learning and data analysis.\" Priyoesh is passionate about technology, and is motivated by the challenges of using technology to solve problems.\"What'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/priyesh2023/GPT\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\n",
    "output = query({\n",
    "\t\"inputs\": \"What is Priyesh's GPA?\",\n",
    "     \"parameters\": {\n",
    "        \"max_length\": 75,\n",
    "        \"temperature\": 0.4,\n",
    "        \"top_k\": 30,\n",
    "        \"top_p\": 0.4,\n",
    "        \"num_beams\": 5,\n",
    "        \"early_stopping\": True,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "        \"do_sample\": True,\n",
    "        \"num_return_sequences\": 1\n",
    "    }\n",
    "})\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
